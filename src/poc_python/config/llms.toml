# params for processors' setup (LLM or other type)
[markup]
approach = "SpacyAndRegexSplitter"
verbose = false
## the next are only if using LLM approach
# n_ctx = 30000
# temperature = 0
# model_path = ""

[key_points]
approach = "LLMKeyPointsExtractor"
n_ctx = 10000
verbose = false
model_path = "/Users/user/Library/Application Support/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf"

[question]
approach = "LLMQuestionGenerator"
n_ctx = 10000
verbose = false
model_path = "/Users/user/Library/Application Support/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf"

[langgraph_split]
approach = "SpacyAndRegexSplitter"
verbose = false

[langgraph_questions]
approach = "LLMQuestionAnswerGenerator"
n_ctx = 10000
verbose = false
model_path = "/Users/user/Library/Application Support/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf"

[langgraph_enrich_question]
approach = "LLMEnrichQuestion"
n_ctx = 1000
verbose = false
model_path = "/Users/user/Library/Application Support/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf"


# params for actual generation
[prompting.key_points]
temperature = 0.9
max_tokens=1000

[prompting.question]
temperature = 0.9
max_tokens=1000
